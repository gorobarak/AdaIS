{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77Wkl0e-X8DZ"
   },
   "source": [
    "Copyright 2025 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK9fZK5x0rMT"
   },
   "source": [
    "<font color='green' size=16>Run CISC and Evaluate</font>\n",
    "\n",
    "This simple notebook runs a model locally to generate answers and confidence score for CISC and compares it to self consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ5cEd4VYuSK"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run before imports so that HF_HOME is set \n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/yandex/APDL2425a/group_12/gorodissky/.cache/huggingface\"\n",
    "print(f\"HF_HOME set to:\\t\\t {os.environ['HF_HOME']}\")\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: \\t{torch.cuda.is_available()}\")\n",
    "print(f\"Torch version: \\t\\t{torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of CUDA devices\\t {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device:\\t\\t {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RVRUSsNtTbcC"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import dataclasses\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from adais.src import confidence_extraction\n",
    "from adais.src import run_lib as run_lib\n",
    "from adais.src.post_processing import aggregators\n",
    "from adais.src.post_processing import run_eval_lib\n",
    "from adais.src.runners import batcher as batcher_lib\n",
    "from adais.src.runners import hugging_face_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AuBQkk92cbKi"
   },
   "outputs": [],
   "source": [
    "# The name or the path to an hugging face model.\n",
    "model_str = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "hf_runner = hugging_face_runner.Runner(model_str)\n",
    "\n",
    "# BATCH_SIZE = 2  # 500 batch size worked with H100. Try smaller for less VRAM.\n",
    "# runner = batcher_lib.BatchRunner(\n",
    "#     hf_runner,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_wait_time_secs=1,\n",
    "# )\n",
    "runner = hf_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nx66B5J4T_31"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_traces = 2   # total number of traces generated per question\n",
    "num_rows = 3  # number of questions\n",
    "max_num_tokens = 756  \n",
    "temp = 0.9  \n",
    "max_workers_stage1 = 3  # More workers as batch size must be 1 here.\n",
    "max_workers_stage2 = 120  \n",
    "tag = model_str.split('/')[-1]  # Use the model name as tag.\n",
    "\n",
    "# Dataset choices are [\"MMLU\", \"GSM8K\", \"MATH\"]\n",
    "dataset_names = [\"MMLU\"]  \n",
    "\n",
    "# Extract the three type of confidence scores report in the paper.\n",
    "confidence_config = confidence_extraction.AggregatedConfidenceExtractionConfig(\n",
    "    verbal_confidence=confidence_extraction.ConfidenceExtractionType.NONE.value,  # HUNDRED.value\n",
    "    confidence_likelihoods=confidence_extraction.ConfidenceExtractionType.BINARY.value,\n",
    "    run_sequence_probability=False,\n",
    ")\n",
    "\n",
    "config = run_lib.ExperimentConfiguration(\n",
    "    num_traces=num_traces,\n",
    "    num_rows=num_rows,\n",
    "    max_num_tokens=max_num_tokens,\n",
    "    temperature=temp,\n",
    "    tag=tag,\n",
    "    confidence_config=confidence_config,\n",
    ")\n",
    "\n",
    "output_base_dir = os.path.join(\"/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/output\", tag)\n",
    "output_base_dir_versioned = os.path.join(\n",
    "    output_base_dir, datetime.datetime.now().strftime(\"%Y_%m_%d_%H:%M\")\n",
    ")\n",
    "\n",
    "all_datasets_results = []\n",
    "# First generate the answers.\n",
    "all_datasets_results.extend(\n",
    "    run_lib.run_question_answering_on_datasets(\n",
    "        runner,\n",
    "        dataset_names,\n",
    "        config=config,\n",
    "        max_workers=max_workers_stage1,\n",
    "        output_base_dir=output_base_dir_versioned,\n",
    "    )\n",
    ")\n",
    "# all_datasets_results = run_lib.load_all_experiment_results(dir_name=output_base_dir)\n",
    "\n",
    "# Next, extract the confidence for each answer.\n",
    "output_base_dir = os.path.join(output_base_dir, \"confidence\")\n",
    "output_base_dir_versioned = os.path.join(\n",
    "    output_base_dir, datetime.datetime.now().strftime(\"%Y_%m_%d_%H:%M\")\n",
    ")\n",
    "all_datasets_results = run_lib.run_confidence_extraction_on_experiment_results(\n",
    "    runner,\n",
    "    all_datasets_results,\n",
    "    config.confidence_config,\n",
    "    max_workers=max_workers_stage2,\n",
    "    output_base_dir=output_base_dir_versioned,\n",
    ")\n",
    "# all_datasets_results = run_lib.load_all_experiment_results(dir_name=output_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bgw9-vQg1-pt"
   },
   "outputs": [],
   "source": [
    "# @title Compute Metrics\n",
    "\n",
    "aggregator_configs = [\n",
    "    aggregators.AggregatorConfig(\n",
    "        aggregator_type=aggregators.AggregatorType.SC,\n",
    "        norm_type=aggregators.NormalizationType.NONE,\n",
    "    ),\n",
    "    aggregators.AggregatorConfig(\n",
    "        aggregator_type=aggregators.AggregatorType.CISC,\n",
    "        norm_type=aggregators.NormalizationType.SOFTMAX,\n",
    "        temperature=0.2,  # This value is taken from the CISC paper.\n",
    "        confidence_col_name=\"logit_confidence\",  # P(True) in the paper.\n",
    "    ),\n",
    "]\n",
    "TRACES_LENS = range(1, num_traces + 1)\n",
    "return_per_question_scores = True  # @param {'type': 'boolean'}\n",
    "datasets_stats = {}\n",
    "for ds in all_datasets_results:\n",
    "  dataset_name = ds.dataset_name\n",
    "  datasets_stats[dataset_name] = (\n",
    "      run_eval_lib.calculate_stats_for_model_and_dataset(\n",
    "          tag,\n",
    "          ds,\n",
    "          dataset_name,\n",
    "          filter_answers=False,\n",
    "          round_negative_conf_to_zero=False,\n",
    "          re_compute_is_correct=True,\n",
    "          aggregator_configs=aggregator_configs,\n",
    "          traces_lens=TRACES_LENS,\n",
    "          num_bootstrap=500,\n",
    "          # Return the score per question. Aggregate latter, in the next cell.\n",
    "          return_per_question_scores=return_per_question_scores,\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "height": 578
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1748297559846,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "ht16qt_SjisT",
    "outputId": "710eb330-d0b5-47fd-96f0-9a2fb6fe01d6"
   },
   "outputs": [],
   "source": [
    "# @title Draw Graphs { vertical-output: true, form-width: \"30%\" }\n",
    "import copy\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_method_display_names_dict(break_type='\\n'):\n",
    "  return {\n",
    "      'verbal_confidence': f'Verbal{break_type}0-100',\n",
    "      'logit_confidence': 'P(True)',\n",
    "      'response_probability': f'Response{break_type}Probability',\n",
    "      'binary_confidence': 'Verbal Binary',\n",
    "  }\n",
    "\n",
    "\n",
    "def aggregate_over_questions(raw_stats):\n",
    "  raw_stats = copy.deepcopy(raw_stats)\n",
    "  for _, dataset_stats in raw_stats.items():\n",
    "    score_stats = dataset_stats.score_stats\n",
    "    updated_stats = {}\n",
    "    for k, v in score_stats.items():\n",
    "      updated_stats[k] = np.array(v).mean(axis=1)\n",
    "    dataset_stats.score_stats = updated_stats\n",
    "  return raw_stats\n",
    "\n",
    "\n",
    "NEED_AGGREGATE_OVER_QUESTIONS = True  # @param {'type': 'boolean'}\n",
    "if NEED_AGGREGATE_OVER_QUESTIONS:\n",
    "  processed_stats = aggregate_over_questions(datasets_stats.copy())\n",
    "else:\n",
    "  processed_stats = datasets_stats\n",
    "\n",
    "target_keys = [\n",
    "    'SC_NONE',\n",
    "    'CISC_verbal_confidence',\n",
    "    'CISC_logit_confidence',\n",
    "    'CISC_response_probability',\n",
    "    'CISC_binary_confidence',\n",
    "]\n",
    "\n",
    "\n",
    "def rename_metrics(\n",
    "    data_to_update: dict,\n",
    "    prefixes_to_keep: list[str],\n",
    "):\n",
    "  \"\"\"Rename the metric to only include the prefixes of `target_keys`\"\"\"\n",
    "  data_to_update = copy.deepcopy(data_to_update)\n",
    "  for _, dataset_stats in data_to_update.items():\n",
    "    updated_score_stats = {}\n",
    "    for method, method_stats in dataset_stats.score_stats.items():\n",
    "      for key in prefixes_to_keep:\n",
    "        if method.startswith(key):\n",
    "          updated_name = key\n",
    "          updated_score_stats[updated_name] = method_stats\n",
    "          break\n",
    "    dataset_stats.score_stats = updated_score_stats\n",
    "  return data_to_update\n",
    "\n",
    "\n",
    "processed_stats = rename_metrics(processed_stats, target_keys)\n",
    "\n",
    "# @title draw graphs\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LabelSettings:\n",
    "  display_name: str\n",
    "  line_type: str\n",
    "  color: str\n",
    "  line_width: int\n",
    "  # If true would draw an horizontal at the height of the last value.\n",
    "  horizontal_line: bool = False\n",
    "\n",
    "\n",
    "labels = {\n",
    "    'SC_NONE': LabelSettings(\n",
    "        'Self-Consistency', 'solid', 'cornflowerblue', 5, False\n",
    "    ),\n",
    "    'CISC_logit_confidence': LabelSettings(\n",
    "        'CISC P(True)', 'solid', 'darkorange', 5\n",
    "    ),\n",
    "    'CISC_response_probability': LabelSettings(\n",
    "        'CISC Response Probability', (0, (3, 2)), 'crimson', 5\n",
    "    ),\n",
    "    'CISC_verbal_confidence': LabelSettings(\n",
    "        'CISC Verbal 1-100', (0, (3, 2)), 'turquoise', 5\n",
    "    ),\n",
    "    'CISC_binary_confidence': LabelSettings(\n",
    "        'CISC Binary', (0, (3, 2)), 'darkviolet', 5\n",
    "    ),\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "\n",
    "def draw_cisc(per_ds_stats, model_name, traces_lens=TRACES_LENS):\n",
    "  i = 0\n",
    "  # per_ds_stats = {'MATH': per_ds_stats['MATH']}\n",
    "  num_cols = len(per_ds_stats)\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=num_cols, figsize=(8 * num_cols, 8))\n",
    "  fig.suptitle(f'{model_name} Accuracy Per Length', fontsize=24)\n",
    "  for dataset_name, stats in per_ds_stats.items():\n",
    "    stats = stats.score_stats\n",
    "    ax = axes[i] if num_cols > 1 else axes\n",
    "    ax.set_title(dataset_name, fontsize=28, weight='semibold')\n",
    "    j = 1\n",
    "    for label, vals in stats.items():\n",
    "      if label not in labels:\n",
    "        continue\n",
    "      label = labels[label]\n",
    "      ax.plot(\n",
    "          traces_lens[:30],\n",
    "          [v * 100 for v in vals[:10]],\n",
    "          linestyle=label.line_type,\n",
    "          linewidth=label.line_width,\n",
    "          label=label.display_name,\n",
    "          color=label.color,\n",
    "          zorder=j,\n",
    "          markersize=10,\n",
    "      )\n",
    "      if label.horizontal_line:\n",
    "        ax.axhline(\n",
    "            y=vals[-1] * 100,\n",
    "            color='mediumseagreen',\n",
    "            linestyle='dashed',\n",
    "            linewidth=4,\n",
    "            xmin=0.26,\n",
    "            xmax=0.96,\n",
    "            marker='s',\n",
    "            markersize=6,\n",
    "            zorder=999,\n",
    "        )\n",
    "      ax.set_xlabel('# Samples', fontsize=24)\n",
    "      ax.set_ylabel('% Accuracy', fontsize=24)\n",
    "      ax.xaxis.set_tick_params(size=0)\n",
    "      ax.yaxis.set_tick_params(size=0)\n",
    "      for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_edgecolor('grey')\n",
    "\n",
    "      ax.grid('on', linestyle='--')\n",
    "      j += 1\n",
    "    ax.legend(fontsize=16)\n",
    "    i += 1\n",
    "  fig.tight_layout()\n",
    "\n",
    "\n",
    "draw_cisc(processed_stats, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "height": 143
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1748297562874,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -180
    },
    "id": "ZGp3ethmknJ4",
    "outputId": "7dd75a3e-ccd3-4daa-beea-f6a5533dca42"
   },
   "outputs": [],
   "source": [
    "# @title Within Question Discrimination - Breakdown { vertical-output: true, form-width: \"30%\" }\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CONFIDENCE_TYPE = \"logit_confidence\"  # @param ['logit_confidence', 'verbal_confidence', 'binary_confidence', 'response_probability']\n",
    "\n",
    "\n",
    "def get_wqd_df(raw_stats, confidence_type):\n",
    "  data = []\n",
    "\n",
    "  for dataset, dataset_stats in raw_stats.items():\n",
    "    cur_stats = dataset_stats.confidence_methods_stats[confidence_type].wqd_eval\n",
    "    score = 0\n",
    "    if cur_stats.num_pairs > 0:\n",
    "      score = (\n",
    "          cur_stats.num_higher_better + cur_stats.num_confidence_ties * 0.5\n",
    "      ) / (cur_stats.num_pairs)\n",
    "    data.append({\n",
    "        \"Model\": tag,\n",
    "        \"Dataset\": dataset,\n",
    "        \"score\": score,\n",
    "        \"higher_cnt\": cur_stats.num_higher_better,\n",
    "    })\n",
    "  df = pd.DataFrame(data)\n",
    "  df = df.pivot(index=\"Model\", columns=\"Dataset\", values=[\"score\"])\n",
    "  return df\n",
    "\n",
    "\n",
    "df = get_wqd_df(processed_stats, CONFIDENCE_TYPE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//experimental/users/amirt/cisc:extended_pytorch_kernel",
    "kind": "private"
   },
   "provenance": [
    {
     "file_id": "1aZ89pwt8Y0WeMdCH-CPeKZoGaitUnFl6",
     "timestamp": 1729966741095
    },
    {
     "file_id": "1QaWspJLTKrfiBKPTgK2JYlAOcv9KnhfA",
     "timestamp": 1725954695778
    }
   ]
  },
  "kernelspec": {
   "display_name": "DIIS venv",
   "language": "python",
   "name": "diis-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

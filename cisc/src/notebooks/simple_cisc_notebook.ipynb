{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Wkl0e-X8DZ"
      },
      "source": [
        "Copyright 2025 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK9fZK5x0rMT"
      },
      "source": [
        "<font color='green' size=16>Run CISC and Evaluate</font>\n",
        "\n",
        "This simple notebook runs a model locally to generate answers and confidence score for CISC and compares it to self consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ5cEd4VYuSK"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Torch version: 2.9.0+cu126\n",
            "Number of CUDA devices: 1\n",
            "CUDA device: NVIDIA H100 80GB HBM3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "RVRUSsNtTbcC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 18:44:23.289087: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 18:44:23.342319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-12-25 18:44:30.075833: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n",
            "/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# @title Imports\n",
        "\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "from cisc.src import confidence_extraction\n",
        "from cisc.src import run_lib as run_lib\n",
        "from cisc.src.post_processing import aggregators\n",
        "from cisc.src.post_processing import run_eval_lib\n",
        "from cisc.src.runners import batcher as batcher_lib\n",
        "from cisc.src.runners import hugging_face_runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AuBQkk92cbKi"
      },
      "outputs": [],
      "source": [
        "# @title Load the Model\n",
        "\n",
        "# The name or the path to an hugging face model.\n",
        "model_str = \"google/gemma-2-2b-it\"\n",
        "hf_runner = hugging_face_runner.Runner(model_str)\n",
        "_MAX_BATCH_SIZE_GEMMA = 500  # This worked with H100. Try smaller for less VRAM.\n",
        "runner = batcher_lib.BatchRunner(\n",
        "    hf_runner,\n",
        "    batch_size=_MAX_BATCH_SIZE_GEMMA,\n",
        "    max_wait_time_secs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nx66B5J4T_31"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# @title Run The Model { form-width: \"25%\"}\n",
        "\n",
        "num_traces = 8  # @param {'type': 'number'}\n",
        "num_rows = 100  # @param {'type': 'number'}\n",
        "max_num_tokens = 756  # @param {'type': 'number'}\n",
        "temp = 0.9  # @param\n",
        "max_workers_stage1 = 2  # @param {'type': 'number'}\n",
        "# More workers as batch size must be 1 here.\n",
        "max_workers_stage2 = 120  # @param {'type': 'number'}\n",
        "tag = \"gemma2_local\"  # @param {'type': 'string'}\n",
        "\n",
        "# Dataset choices are [\"MMLU\", \"GSM8K\", \"MATH\"]\n",
        "dataset_names = [\"BBH\"]  # @param\n",
        "\n",
        "# Extract the three type of confidence scores report in the paper.\n",
        "confidence_config = confidence_extraction.AggregatedConfidenceExtractionConfig(\n",
        "    verbal_confidence=confidence_extraction.ConfidenceExtractionType.NONE.value,  # HUNDRED.value,\n",
        "    confidence_likelihoods=confidence_extraction.ConfidenceExtractionType.BINARY.value,\n",
        "    run_sequence_probability=False,\n",
        ")\n",
        "\n",
        "config = run_lib.ExperimentConfiguration(\n",
        "    num_traces,\n",
        "    num_rows,\n",
        "    max_num_tokens=max_num_tokens,\n",
        "    temperature=temp,\n",
        "    tag=tag,\n",
        "    confidence_config=confidence_config,\n",
        ")\n",
        "\n",
        "output_base_dir = os.path.join(\"/tmp/cisc/\", tag)\n",
        "output_base_dir_versioned = os.path.join(\n",
        "    output_base_dir, datetime.datetime.now().strftime(\"%Y_%m_%d_%H:%M\")\n",
        ")\n",
        "\n",
        "all_datasets_results = []\n",
        "# First generate the answers.\n",
        "all_datasets_results.extend(\n",
        "    run_lib.run_question_answering_on_datasets(\n",
        "        runner,\n",
        "        dataset_names,\n",
        "        config=config,\n",
        "        max_workers=max_workers_stage1,\n",
        "        output_base_dir=output_base_dir_versioned,\n",
        "    )\n",
        ")\n",
        "# all_datasets_results = run_lib.load_all_experiment_results(dir_name=output_base_dir)\n",
        "\n",
        "# Next, extract the confidence for each answer.\n",
        "output_base_dir = os.path.join(output_base_dir, \"confidence\")\n",
        "output_base_dir_versioned = os.path.join(\n",
        "    output_base_dir, datetime.datetime.now().strftime(\"%Y_%m_%d_%H:%M\")\n",
        ")\n",
        "all_datasets_results = run_lib.run_confidence_extraction_on_experiment_results(\n",
        "    runner,\n",
        "    all_datasets_results,\n",
        "    config.confidence_config,\n",
        "    max_workers=max_workers_stage2,\n",
        "    output_base_dir=output_base_dir_versioned,\n",
        ")\n",
        "# all_datasets_results = run_lib.load_all_experiment_results(dir_name=output_base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bgw9-vQg1-pt"
      },
      "outputs": [],
      "source": [
        "# @title Compute Metrics\n",
        "\n",
        "aggregator_configs = [\n",
        "    aggregators.AggregatorConfig(\n",
        "        aggregator_type=aggregators.AggregatorType.SC,\n",
        "        norm_type=aggregators.NormalizationType.NONE,\n",
        "    ),\n",
        "    aggregators.AggregatorConfig(\n",
        "        aggregator_type=aggregators.AggregatorType.CISC,\n",
        "        norm_type=aggregators.NormalizationType.SOFTMAX,\n",
        "        temperature=0.2,  # This value is taken from the CISC paper.\n",
        "        confidence_col_name=\"logit_confidence\",  # P(True) in the paper.\n",
        "    ),\n",
        "]\n",
        "TRACES_LENS = range(1, num_traces + 1)\n",
        "return_per_question_scores = True  # @param {'type': 'boolean'}\n",
        "datasets_stats = {}\n",
        "for ds in all_datasets_results:\n",
        "  dataset_name = ds.dataset_name\n",
        "  datasets_stats[dataset_name] = (\n",
        "      run_eval_lib.calculate_stats_for_model_and_dataset(\n",
        "          tag,\n",
        "          ds,\n",
        "          dataset_name,\n",
        "          filter_answers=False,\n",
        "          round_negative_conf_to_zero=False,\n",
        "          re_compute_is_correct=True,\n",
        "          aggregator_configs=aggregator_configs,\n",
        "          traces_lens=TRACES_LENS,\n",
        "          num_bootstrap=500,\n",
        "          # Return the score per question. Aggregate latter, in the next cell.\n",
        "          return_per_question_scores=return_per_question_scores,\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "height": 578
        },
        "executionInfo": {
          "elapsed": 256,
          "status": "ok",
          "timestamp": 1748297559846,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -180
        },
        "id": "ht16qt_SjisT",
        "outputId": "710eb330-d0b5-47fd-96f0-9a2fb6fe01d6"
      },
      "outputs": [],
      "source": [
        "# @title Draw Graphs { vertical-output: true, form-width: \"30%\" }\n",
        "import copy\n",
        "import dataclasses\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_method_display_names_dict(break_type='\\n'):\n",
        "  return {\n",
        "      'verbal_confidence': f'Verbal{break_type}0-100',\n",
        "      'logit_confidence': 'P(True)',\n",
        "      'response_probability': f'Response{break_type}Probability',\n",
        "      'binary_confidence': 'Verbal Binary',\n",
        "  }\n",
        "\n",
        "\n",
        "def aggregate_over_questions(raw_stats):\n",
        "  raw_stats = copy.deepcopy(raw_stats)\n",
        "  for _, dataset_stats in raw_stats.items():\n",
        "    score_stats = dataset_stats.score_stats\n",
        "    updated_stats = {}\n",
        "    for k, v in score_stats.items():\n",
        "      updated_stats[k] = np.array(v).mean(axis=1)\n",
        "    dataset_stats.score_stats = updated_stats\n",
        "  return raw_stats\n",
        "\n",
        "\n",
        "NEED_AGGREGATE_OVER_QUESTIONS = True  # @param {'type': 'boolean'}\n",
        "if NEED_AGGREGATE_OVER_QUESTIONS:\n",
        "  processed_stats = aggregate_over_questions(datasets_stats.copy())\n",
        "else:\n",
        "  processed_stats = datasets_stats\n",
        "\n",
        "target_keys = [\n",
        "    'SC_NONE',\n",
        "    'CISC_verbal_confidence',\n",
        "    'CISC_logit_confidence',\n",
        "    'CISC_response_probability',\n",
        "    'CISC_binary_confidence',\n",
        "]\n",
        "\n",
        "\n",
        "def rename_metrics(\n",
        "    data_to_update: dict,\n",
        "    prefixes_to_keep: list[str],\n",
        "):\n",
        "  \"\"\"Rename the metric to only include the prefixes of `target_keys`\"\"\"\n",
        "  data_to_update = copy.deepcopy(data_to_update)\n",
        "  for _, dataset_stats in data_to_update.items():\n",
        "    updated_score_stats = {}\n",
        "    for method, method_stats in dataset_stats.score_stats.items():\n",
        "      for key in prefixes_to_keep:\n",
        "        if method.startswith(key):\n",
        "          updated_name = key\n",
        "          updated_score_stats[updated_name] = method_stats\n",
        "          break\n",
        "    dataset_stats.score_stats = updated_score_stats\n",
        "  return data_to_update\n",
        "\n",
        "\n",
        "processed_stats = rename_metrics(processed_stats, target_keys)\n",
        "\n",
        "# @title draw graphs\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class LabelSettings:\n",
        "  display_name: str\n",
        "  line_type: str\n",
        "  color: str\n",
        "  line_width: int\n",
        "  # If true would draw an horizontal at the height of the last value.\n",
        "  horizontal_line: bool = False\n",
        "\n",
        "\n",
        "labels = {\n",
        "    'SC_NONE': LabelSettings(\n",
        "        'Self-Consistency', 'solid', 'cornflowerblue', 5, False\n",
        "    ),\n",
        "    'CISC_logit_confidence': LabelSettings(\n",
        "        'CISC P(True)', 'solid', 'darkorange', 5\n",
        "    ),\n",
        "    'CISC_response_probability': LabelSettings(\n",
        "        'CISC Response Probability', (0, (3, 2)), 'crimson', 5\n",
        "    ),\n",
        "    'CISC_verbal_confidence': LabelSettings(\n",
        "        'CISC Verbal 1-100', (0, (3, 2)), 'turquoise', 5\n",
        "    ),\n",
        "    'CISC_binary_confidence': LabelSettings(\n",
        "        'CISC Binary', (0, (3, 2)), 'darkviolet', 5\n",
        "    ),\n",
        "}\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "\n",
        "\n",
        "def draw_cisc(per_ds_stats, model_name, traces_lens=TRACES_LENS):\n",
        "  i = 0\n",
        "  # per_ds_stats = {'MATH': per_ds_stats['MATH']}\n",
        "  num_cols = len(per_ds_stats)\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=num_cols, figsize=(8 * num_cols, 8))\n",
        "  fig.suptitle(f'{model_name} Accuracy Per Length', fontsize=24)\n",
        "  for dataset_name, stats in per_ds_stats.items():\n",
        "    stats = stats.score_stats\n",
        "    ax = axes[i] if num_cols > 1 else axes\n",
        "    ax.set_title(dataset_name, fontsize=28, weight='semibold')\n",
        "    j = 1\n",
        "    for label, vals in stats.items():\n",
        "      if label not in labels:\n",
        "        continue\n",
        "      label = labels[label]\n",
        "      ax.plot(\n",
        "          traces_lens[:30],\n",
        "          [v * 100 for v in vals[:10]],\n",
        "          linestyle=label.line_type,\n",
        "          linewidth=label.line_width,\n",
        "          label=label.display_name,\n",
        "          color=label.color,\n",
        "          zorder=j,\n",
        "          markersize=10,\n",
        "      )\n",
        "      if label.horizontal_line:\n",
        "        ax.axhline(\n",
        "            y=vals[-1] * 100,\n",
        "            color='mediumseagreen',\n",
        "            linestyle='dashed',\n",
        "            linewidth=4,\n",
        "            xmin=0.26,\n",
        "            xmax=0.96,\n",
        "            marker='s',\n",
        "            markersize=6,\n",
        "            zorder=999,\n",
        "        )\n",
        "      ax.set_xlabel('# Samples', fontsize=24)\n",
        "      ax.set_ylabel('% Accuracy', fontsize=24)\n",
        "      ax.xaxis.set_tick_params(size=0)\n",
        "      ax.yaxis.set_tick_params(size=0)\n",
        "      for spine in ax.spines.values():\n",
        "        spine.set_linewidth(2)\n",
        "        spine.set_edgecolor('grey')\n",
        "\n",
        "      ax.grid('on', linestyle='--')\n",
        "      j += 1\n",
        "    ax.legend(fontsize=16)\n",
        "    i += 1\n",
        "  fig.tight_layout()\n",
        "\n",
        "\n",
        "draw_cisc(processed_stats, tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "height": 143
        },
        "executionInfo": {
          "elapsed": 58,
          "status": "ok",
          "timestamp": 1748297562874,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -180
        },
        "id": "ZGp3ethmknJ4",
        "outputId": "7dd75a3e-ccd3-4daa-beea-f6a5533dca42"
      },
      "outputs": [],
      "source": [
        "# @title Within Question Discrimination - Breakdown { vertical-output: true, form-width: \"30%\" }\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "CONFIDENCE_TYPE = \"logit_confidence\"  # @param ['logit_confidence', 'verbal_confidence', 'binary_confidence', 'response_probability']\n",
        "\n",
        "\n",
        "def get_wqd_df(raw_stats, confidence_type):\n",
        "  data = []\n",
        "\n",
        "  for dataset, dataset_stats in raw_stats.items():\n",
        "    cur_stats = dataset_stats.confidence_methods_stats[confidence_type].wqd_eval\n",
        "    score = 0\n",
        "    if cur_stats.num_pairs > 0:\n",
        "      score = (\n",
        "          cur_stats.num_higher_better + cur_stats.num_confidence_ties * 0.5\n",
        "      ) / (cur_stats.num_pairs)\n",
        "    data.append({\n",
        "        \"Model\": tag,\n",
        "        \"Dataset\": dataset,\n",
        "        \"score\": score,\n",
        "        \"higher_cnt\": cur_stats.num_higher_better,\n",
        "    })\n",
        "  df = pd.DataFrame(data)\n",
        "  df = df.pivot(index=\"Model\", columns=\"Dataset\", values=[\"score\"])\n",
        "  return df\n",
        "\n",
        "\n",
        "df = get_wqd_df(processed_stats, CONFIDENCE_TYPE)\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//experimental/users/amirt/cisc:extended_pytorch_kernel",
        "kind": "private"
      },
      "provenance": [
        {
          "file_id": "1aZ89pwt8Y0WeMdCH-CPeKZoGaitUnFl6",
          "timestamp": 1729966741095
        },
        {
          "file_id": "1QaWspJLTKrfiBKPTgK2JYlAOcv9KnhfA",
          "timestamp": 1725954695778
        }
      ]
    },
    "kernelspec": {
      "display_name": "DIIS venv",
      "language": "python",
      "name": "diis-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

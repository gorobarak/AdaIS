{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488f1fa7",
   "metadata": {},
   "source": [
    "# Train correctness probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1fe26",
   "metadata": {},
   "source": [
    "## Requierments:\n",
    "1. prompts the model to answer questions from `dataset`\n",
    "2. record question's last token actiavtion from mid layers (50%)\n",
    "3. Label every question as correct/ incorrect depending whether the model got it right or not\n",
    "    1.  Uses LLM as a judge\n",
    "4. partition the activations to correct and incorrect sets\n",
    "5. Calculates the direction from the correct group to the incorrect group\n",
    "    1. Translate vectors to have as origin the mean of the two group's centriods, call it $o$\n",
    "    2. Calcluate the mean direction from $o$ to correct, $\\mu_{\\text{correct}}$\n",
    "    3. Calcluate the mean direction from $o$ to incorrect, $\\mu_{\\text{incorrect}}$\n",
    "    4. return $0.5 \\cdot (\\mu_{\\text{correct}} - \\mu_{\\text{incorrect}})$\n",
    "6. Construct scorer class with\n",
    "    1. The direction from incorret to corret $\\mu$\n",
    "    2. The new origin vector $o$\n",
    "    3. A score method that computes for hidden state $h$, score(h) $= \\frac{1}{\\lVert \\mu \\rVert}\\mu^T \\cdot \\left(h - o \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417ecc3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20d1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME set to:\t\t /home/yandex/APDL2425a/group_12/gorodissky/.cache/huggingface\n",
      "CUDA available: \tTrue\n",
      "Torch version: \t\t2.9.0+cu126\n",
      "Number of CUDA devices\t 4\n",
      "CUDA device:\t\t NVIDIA GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "# Run before imports so that HF_HOME is set \n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/yandex/APDL2425a/group_12/gorodissky/.cache/huggingface\"\n",
    "print(f\"HF_HOME set to:\\t\\t {os.environ['HF_HOME']}\")\n",
    "\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "\n",
    "print(f\"CUDA available: \\t{torch.cuda.is_available()}\")\n",
    "print(f\"Torch version: \\t\\t{torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of CUDA devices\\t {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device:\\t\\t {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556156f",
   "metadata": {},
   "source": [
    "## Choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c62c1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str  = \"Qwen/Qwen2.5-0.5B-Instruct\" #\"google/gemma-2-2b-it\"  hf model \n",
    "judge_model_name: str = \"gpt-4o-mini\"  # model used for judging answers\n",
    "base_save_dir : str = \"/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/output\"  # base directory to save outputs\n",
    "dataset_name: tuple[str, str] =  (\"mandarjoshi/trivia_qa\", \"rc\") # hf dataset name and subset\n",
    "dataset_size: int = 16\n",
    "seed: int = 1337\n",
    "batch_size: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2be9eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a11c7ea8",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6553908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ('mandarjoshi/trivia_qa', 'rc')\n",
      "Dataset loaded: 16 examples\n",
      "\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Model loaded on device: cuda:1 dtype: torch.bfloat16\n",
      "Model has 24 layers\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {dataset_name}\")\n",
    "ds = load_dataset(*dataset_name, split=\"validation\", streaming=False)\n",
    "ds = ds.shuffle(seed=seed).select(range(dataset_size))\n",
    "print(f\"Dataset loaded: {len(ds)} examples\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"\\nLoading model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded on device: {model.device} dtype: {model.dtype}\")\n",
    "print(f\"Model has {model.config.num_hidden_layers} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58875",
   "metadata": {},
   "source": [
    "### See what type of parametes are passed into hook function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fccdd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(module, input, output):\n",
    "    print(\"----- Hook called -----\")\n",
    "    print(f\"Inside {module.__class__.__name__} forward\")\n",
    "    print(f\"Input type: {type(input)}\")\n",
    "    if isinstance(input, tuple):\n",
    "        for i, inp in enumerate(input):\n",
    "            print(f\"Input {i} type: {type(inp)}\")\n",
    "            if isinstance(inp, torch.Tensor):\n",
    "                print(f\"Input {i} shape: {inp.shape}\")\n",
    "    else:\n",
    "        print(f\"Input shape: {input.shape}\")\n",
    "    print(f\"Output type: {type(output)}\")\n",
    "    if isinstance(output, tuple):\n",
    "        for i, out in enumerate(output):\n",
    "            print(f\"Output {i} type: {type(out)}\")\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                print(f\"Output {i} shape: {out.shape}\")\n",
    "    else:\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\n",
    "# Register hook on the first transformer layer\n",
    "first_layer = model.model.layers[0]\n",
    "hook_handle = first_layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d617ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single example\n",
    "question = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    question,\n",
    "    tokeinze=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs.to(model.device), max_new_tokens=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "113c54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: remove hooks\n",
    "for layer_indx in range(model.config.num_hidden_layers):\n",
    "    model.model.layers[layer_indx]._forward_hooks.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e382ebd",
   "metadata": {},
   "source": [
    "## Set up activation capture for mid layers (50%) and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1692adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing activations from layers [11, 12, 13] (around 50% depth)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First cleanup\n",
    "num_layers = model.config.num_hidden_layers\n",
    "for layer_indx in range(num_layers):\n",
    "    model.model.layers[layer_indx]._forward_hooks.clear()\n",
    "mid_layer_idx = num_layers // 2\n",
    "layer_indices = [mid_layer_idx - 1, mid_layer_idx, mid_layer_idx + 1]\n",
    "print(f\"Capturing activations from layers {layer_indices} (around 50% depth)\")\n",
    "\n",
    "# Storage for activations from each layer\n",
    "activations_storage_minus1 = []\n",
    "activations_storage_mid = []\n",
    "activations_storage_plus1 = []\n",
    "\n",
    "def activation_hook_minus1(module, input, output):\n",
    "    \"\"\"Hook to capture the last token's hidden state from layer mid-1\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0] # if output is a tuple, first element is hidden states\n",
    "    last_token_activation = output[:, -1, :].detach().cpu()\n",
    "    activations_storage_minus1.append(last_token_activation)\n",
    "\n",
    "def activation_hook_mid(module, input, output):\n",
    "    \"\"\"Hook to capture the last token's hidden state from middle layer\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0] # if output is a tuple, first element is hidden states\n",
    "    last_token_activation = output[:, -1, :].detach().cpu()\n",
    "    activations_storage_mid.append(last_token_activation)\n",
    "\n",
    "def activation_hook_plus1(module, input, output):\n",
    "    \"\"\"Hook to capture the last token's hidden state from layer mid+1\"\"\"\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0] # if output is a tuple, first element is hidden states\n",
    "    last_token_activation = output[:, -1, :].detach().cpu()\n",
    "    activations_storage_plus1.append(last_token_activation)\n",
    "\n",
    "# Register hooks on the three layers\n",
    "hook_handle_minus1 = model.model.layers[mid_layer_idx - 1].register_forward_hook(activation_hook_minus1)\n",
    "hook_handle_mid = model.model.layers[mid_layer_idx].register_forward_hook(activation_hook_mid)\n",
    "hook_handle_plus1 = model.model.layers[mid_layer_idx + 1].register_forward_hook(activation_hook_plus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42ce85",
   "metadata": {},
   "source": [
    "## Generate responses and collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba4fc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses with batch_size=2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses with batch_size=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:13<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses with batch_size=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:13<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 16 Q&A pairs\n",
      "Activations shape: torch.Size([16, 896]) (Mean of layers [11, 12, 13] from prompt's last token)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nGenerating responses with batch_size={batch_size}...\")\n",
    "\n",
    "prompt_template =\"\"\"I am going to ask you a question. Answer concisely. End your sentence with {eos_token}. Here\n",
    "are some examples of questions that might help you:\n",
    "—--\n",
    "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
    "Answer: Sinclair Lewis{eos_token}\n",
    "—--\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "# print(prompt_template.format(eos_token=\"<|eos|>\", question=\"Which American-born Sinclair won the Nobel Prize for Literature in 1930?\"))\n",
    "all_questions = []\n",
    "all_ground_truths = []\n",
    "all_model_answers = []\n",
    "all_activations = []\n",
    "\n",
    "for i in tqdm(range(0, len(ds), batch_size)):\n",
    "    batch = ds[i:min(i + batch_size, len(ds))]\n",
    "    \n",
    "    # Prepare questions\n",
    "    questions = batch['question']\n",
    "    all_questions.extend(questions)\n",
    "    \n",
    "    # Store ground truth\n",
    "    ground_truths = [ans['value'][0] if isinstance(ans['value'], list) else ans['value'] \n",
    "                     for ans in batch['answer']]\n",
    "    all_ground_truths.extend(ground_truths)\n",
    "    \n",
    "    # Format prompts\n",
    "    prompts = [prompt_template.format(eos_token=tokenizer.eos_token, question=q) for q in questions]\n",
    "    prompts = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer.padding_side = \"left\" # last token is the prompt's \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompts, tokeinze=True, \n",
    "        add_generation_prompt=True, \n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # # DEBUG: print proccessed prompt\n",
    "    # print(f\"procced prompts shape: {inputs['input_ids'].shape}\")\n",
    "    # print(\"attention mask:\\n\", inputs['attention_mask'])\n",
    "    # procced_prompts =tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=False)\n",
    "    # print(\"procced prompts:\")\n",
    "    # for pp in procced_prompts:\n",
    "    #     print(repr(pp))\n",
    "    # break\n",
    "    \n",
    "    # Clear previous activations\n",
    "    activations_storage_minus1.clear()\n",
    "    activations_storage_mid.clear()\n",
    "    activations_storage_plus1.clear()\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False, # greedy decoding matching the paper's setup\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # # DEBUG: print responses\n",
    "    # print(type(outputs))\n",
    "    # answers_tokens = outputs[:, inputs[\"input_ids\"].size(1):]\n",
    "    # answers = tokenizer.batch_decode(answers_tokens, skip_special_tokens=False)\n",
    "    # for a in answers:\n",
    "    #     print(a)\n",
    "    # break\n",
    "\n",
    "    # Extract answers (remove prompt)\n",
    "    prompt_lengths = inputs['input_ids'].shape[1]\n",
    "    generated_tokens = outputs[:, prompt_lengths:]\n",
    "    answers = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    all_model_answers.extend(answers)\n",
    "    \n",
    "    # Store activations from the FIRST forward pass (the prompt processing)\n",
    "    # The first forward pass processes the full prompt and the hooks captures the last token activation\n",
    "    # The subsequent forward passes proccess the new generated tokens only each time sequence of length 1\n",
    "    if activations_storage_minus1 and activations_storage_mid and activations_storage_plus1:\n",
    "        act_minus1 = activations_storage_minus1[0]\n",
    "        act_mid = activations_storage_mid[0]\n",
    "        act_plus1 = activations_storage_plus1[0]\n",
    "        \n",
    "        # Compute mean across the 3 layers\n",
    "        mean_activation = torch.stack([act_minus1, act_mid, act_plus1], dim=0).mean(dim=0)\n",
    "        all_activations.append(mean_activation)\n",
    "\n",
    "# Remove hooks\n",
    "hook_handle_minus1.remove()\n",
    "hook_handle_mid.remove()\n",
    "hook_handle_plus1.remove()\n",
    "\n",
    "# Concatenate all activations\n",
    "activations_tensor = torch.cat(all_activations, dim=0)\n",
    "print(f\"\\nCollected {len(all_questions)} Q&A pairs\")\n",
    "print(f\"Activations shape: {activations_tensor.shape} (Mean of layers {layer_indices} from prompt's last token)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e329d",
   "metadata": {},
   "source": [
    "## LLM as a judge to label correct/incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "597941c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating answers using LLM as judge...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating answers using LLM as judge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Judging:   0%|          | 0/16 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating answers using LLM as judge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Judging:   0%|          | 0/16 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'You do not have access to the project tied to the API key.', 'type': 'invalid_request_error', 'code': 'invalid_project', 'param': None}, 'status': 401}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (q, gt, ma) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[38;5;28mzip\u001b[39m(all_questions, all_ground_truths, all_model_answers), \n\u001b[32m     34\u001b[39m                                       total=\u001b[38;5;28mlen\u001b[39m(all_questions), desc=\u001b[33m\"\u001b[39m\u001b[33mJudging\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     35\u001b[39m     judge_prompt = judge_prompt_template.format(\n\u001b[32m     36\u001b[39m         question=q,\n\u001b[32m     37\u001b[39m         ground_truth=gt,\n\u001b[32m     38\u001b[39m         model_answer=ma\n\u001b[32m     39\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     judgment = \u001b[43mquery_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjudge_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjudge_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     is_correct = \u001b[33m\"\u001b[39m\u001b[33mCORRECT\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m judgment.upper() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mINCORRECT\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m judgment.upper()\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# DEBUG: print prompt and judgment\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mquery_gpt\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_gpt\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4.1-nano\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      7\u001b[39m     client = openai.OpenAI()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     text = response.output_text.strip()\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:866\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    829\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    830\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    865\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'You do not have access to the project tied to the API key.', 'type': 'invalid_request_error', 'code': 'invalid_project', 'param': None}, 'status': 401}"
     ]
    }
   ],
   "source": [
    "with open(\"/home/yandex/APDL2425a/group_12/gorodissky/tokens/openai_key_personal.txt\", \"r\") as f:\n",
    "    openai_key = f.read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "import openai\n",
    "\n",
    "def query_gpt(prompt, model=\"gpt-4.1-nano\"):\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    text = response.output_text.strip()\n",
    "    return text\n",
    "\n",
    "print(\"\\nEvaluating answers using LLM as judge...\")\n",
    "\n",
    "judge_prompt_template = \"\"\"You are evaluating whether a model's answer is correct.\n",
    "\n",
    "Question: {question}\n",
    "Ground Truth: {ground_truth}\n",
    "Model Answer: {model_answer}\n",
    "\n",
    "Is the model's answer correct? Consider semantic equivalence, not just exact match. Think step by step.\n",
    "Respond with only \"CORRECT\" or \"INCORRECT\".\n",
    "\n",
    "Judgment:\"\"\"\n",
    "\n",
    "correctness_labels = []\n",
    "\n",
    "for i, (q, gt, ma) in enumerate(tqdm(zip(all_questions, all_ground_truths, all_model_answers), \n",
    "                                      total=len(all_questions), desc=\"Judging\")):\n",
    "    judge_prompt = judge_prompt_template.format(\n",
    "        question=q,\n",
    "        ground_truth=gt,\n",
    "        model_answer=ma\n",
    "    )\n",
    "    judgment = query_gpt(judge_prompt, model=judge_model_name)\n",
    "    is_correct = \"CORRECT\" in judgment.upper() and \"INCORRECT\" not in judgment.upper()\n",
    "    # # DEBUG: print prompt and judgment\n",
    "    # print(\"\\n--- Judge Prompt ---\")\n",
    "    # print(judge_prompt)\n",
    "    # print(\"--- Judgment ---\")\n",
    "    # print(judgment)\n",
    "    # print(f\"Is correct: {is_correct}\")\n",
    "    correctness_labels.append(is_correct)\n",
    "\n",
    "correctness_array = np.array(correctness_labels)\n",
    "num_correct = correctness_array.sum()\n",
    "num_incorrect = len(correctness_array) - num_correct\n",
    "\n",
    "print(f\"\\nCorrect: {num_correct} ({100*num_correct/len(correctness_array):.1f}%)\")\n",
    "print(f\"Incorrect: {num_incorrect} ({100*num_incorrect/len(correctness_array):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculating probe direction...\")\n",
    "\n",
    "# Convert to numpy for easier computation\n",
    "activations_np = activations_tensor.to(torch.float32).numpy()\n",
    "\n",
    "# Partition into correct and incorrect\n",
    "correct_mask = correctness_array\n",
    "incorrect_mask = ~correctness_array\n",
    "\n",
    "correct_activations = activations_np[correct_mask]\n",
    "incorrect_activations = activations_np[incorrect_mask]\n",
    "\n",
    "print(f\"Correct activations: {correct_activations.shape}\")\n",
    "print(f\"Incorrect activations: {incorrect_activations.shape}\")\n",
    "\n",
    "# Calculate centroids\n",
    "correct_centroid = correct_activations.mean(axis=0)\n",
    "incorrect_centroid = incorrect_activations.mean(axis=0)\n",
    "\n",
    "# New origin is the midpoint between centroids\n",
    "new_origin = (correct_centroid + incorrect_centroid) / 2\n",
    "\n",
    "# Translate to new origin\n",
    "correct_centered = correct_activations - new_origin\n",
    "incorrect_centered = incorrect_activations - new_origin\n",
    "\n",
    "# Calculate mean directions from new origin\n",
    "mu_correct = correct_centered.mean(axis=0)\n",
    "mu_incorrect = incorrect_centered.mean(axis=0)\n",
    "\n",
    "# Direction from incorrect to correct\n",
    "direction = 0.5 * (mu_correct - mu_incorrect)\n",
    "\n",
    "print(f\"\\nDirection vector shape: {direction.shape}\")\n",
    "print(f\"Direction vector norm: {np.linalg.norm(direction):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectnessScorer:\n",
    "    \"\"\"\n",
    "    A scorer that projects hidden states onto the correctness direction.\n",
    "    \n",
    "    Attributes:\n",
    "        direction (np.ndarray): The direction vector from incorrect to correct (μ)\n",
    "        origin (np.ndarray): The new origin vector (o)\n",
    "        direction_norm (float): The norm of the direction vector\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, direction: np.ndarray, origin: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the scorer with a direction and origin.\n",
    "        \n",
    "        Args:\n",
    "            direction: The direction vector from incorrect to correct\n",
    "            origin: The origin vector (midpoint of centroids)\n",
    "        \"\"\"\n",
    "        self.direction = direction\n",
    "        self.origin = origin\n",
    "        self.direction_norm = np.linalg.norm(direction)\n",
    "        \n",
    "    def score(self, hidden_state: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the correctness score for a hidden state.\n",
    "        \n",
    "        Formula: score(h) = (1 / ||μ||) * μ^T · (h - o)\n",
    "        \n",
    "        Args:\n",
    "            hidden_state: The hidden state vector to score\n",
    "            \n",
    "        Returns:\n",
    "            The correctness score (higher means more likely correct)\n",
    "        \"\"\"\n",
    "        if hidden_state.ndim == 1:\n",
    "            # Single vector\n",
    "            centered = hidden_state - self.origin\n",
    "            score = np.dot(self.direction, centered) / self.direction_norm\n",
    "            return float(score)\n",
    "        else:\n",
    "            # Batch of vectors\n",
    "            centered = hidden_state - self.origin\n",
    "            scores = np.dot(centered, self.direction) / self.direction_norm\n",
    "            return scores\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save the scorer to disk.\"\"\"\n",
    "        np.savez(filepath, direction=self.direction, origin=self.origin)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        \"\"\"Load a scorer from disk.\"\"\"\n",
    "        data = np.load(filepath)\n",
    "        return cls(direction=data['direction'], origin=data['origin'])\n",
    "\n",
    "# Create the scorer\n",
    "scorer = CorrectnessScorer(direction=direction, origin=new_origin)\n",
    "print(f\"  Scorer created successfully\")\n",
    "print(f\"  Direction norm: {scorer.direction_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e40a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTesting scorer on training data...\")\n",
    "\n",
    "# Score all activations\n",
    "scores = scorer.score(activations_np)\n",
    "\n",
    "# Analyze score distribution\n",
    "correct_scores = scores[correct_mask]\n",
    "incorrect_scores = scores[incorrect_mask]\n",
    "\n",
    "print(f\"\\nCorrect answers:\")\n",
    "print(f\"  Mean score: {correct_scores.mean():.4f}\")\n",
    "print(f\"  Std score: {correct_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\nIncorrect answers:\")\n",
    "print(f\"  Mean score: {incorrect_scores.mean():.4f}\")\n",
    "print(f\"  Std score: {incorrect_scores.std():.4f}\")\n",
    "\n",
    "# Calculate separation\n",
    "separation = np.abs(correct_scores.mean() - incorrect_scores.mean())\n",
    "print(f\"\\nSeparation (distance between means): {separation:.4f}\")\n",
    "\n",
    "# Simple threshold classification (at midpoint)\n",
    "threshold = (correct_scores.mean() + incorrect_scores.mean()) / 2\n",
    "predicted_correct = scores > threshold\n",
    "accuracy = (predicted_correct == correctness_array).mean()\n",
    "print(f\"Training accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "save_dir = Path(base_save_dir) \n",
    "save_dir = save_dir / \"probe_results\" / dataset_name[0] / model_name \n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the scorer\n",
    "version = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "scorer_path = save_dir / f\"correctness_scorer_{version}.npz\"\n",
    "scorer.save(str(scorer_path))\n",
    "\n",
    "# Save metadata and results\n",
    "metadata = {\n",
    "    \"model_name\": model_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"dataset_size\": dataset_size,\n",
    "    \"seed\": seed,\n",
    "    \"layers_idx\": layer_indices,\n",
    "    \"correct_score_mean\": float(correct_scores.mean()),\n",
    "    \"correct_score_std\": float(correct_scores.std()),\n",
    "    \"incorrect_score_mean\": float(incorrect_scores.mean()),\n",
    "    \"incorrect_score_std\": float(incorrect_scores.std()),\n",
    "    \"separation\": float(separation),\n",
    "    \"training_accuracy\": float(accuracy),\n",
    "    \"threshold (midpoint between means)\": float(threshold),\n",
    "}\n",
    "\n",
    "metadata_path = save_dir / f\"metadata_{version}.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\" Results saved to {save_dir}\")\n",
    "print(f\"  - Scorer: {scorer_path}\")\n",
    "print(f\"  - Metadata: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541f9aa",
   "metadata": {},
   "source": [
    "## Example: Using the Scorer\n",
    "\n",
    "Here's how to use the trained scorer on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_scorer = CorrectnessScorer.load(str(scorer_path))\n",
    "print(f\"Example usage of loaded scorer:\")\n",
    "for i in range(min(activations_np.shape[0], 100)):\n",
    "    test_activation = activations_np[i]\n",
    "    test_score = loaded_scorer.score(test_activation)\n",
    "    print(f\" example ({i}) correctness score : {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd5ed7",
   "metadata": {},
   "source": [
    "# Run whole pipline\n",
    "First set config parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d530ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIIS venv",
   "language": "python",
   "name": "diis-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

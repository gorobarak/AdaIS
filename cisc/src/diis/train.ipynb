{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488f1fa7",
   "metadata": {},
   "source": [
    "# Train correctness probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1fe26",
   "metadata": {},
   "source": [
    "## Requierments:\n",
    "1. prompts the model to answer questions from `dataset`\n",
    "2. record question's last token actiavtion from mid layers (50%)\n",
    "3. Label every question as correct/ incorrect depending whether the model got it right or not\n",
    "    1.  Uses LLM as a judge\n",
    "4. partition the activations to correct and incorrect sets\n",
    "5. Calculates the direction from the correct group to the incorrect group\n",
    "    1. Translate vectors to have as origin the mean of the two group's centriods, call it $o$\n",
    "    2. Calcluate the mean direction from $o$ to correct, $\\mu_{\\text{correct}}$\n",
    "    3. Calcluate the mean direction from $o$ to incorrect, $\\mu_{\\text{incorrect}}$\n",
    "    4. return $0.5 \\cdot (\\mu_{\\text{correct}} - \\mu_{\\text{incorrect}})$\n",
    "6. Construct scorer class with\n",
    "    1. The direction from incorret to corret $\\mu$\n",
    "    2. The new origin vector $o$\n",
    "    3. A score method that computes for hidden state $h$, score(h) $= \\frac{1}{\\lVert \\mu \\rVert}\\mu^T \\cdot \\left(h - o \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417ecc3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20d1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME set to:\t\t /home/yandex/APDL2425a/group_12/gorodissky/.cache/huggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: \tTrue\n",
      "Torch version: \t\t2.9.0+cu126\n",
      "Number of CUDA devices\t 1\n",
      "CUDA device:\t\t NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Run before imports so that HF_HOME is set\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/yandex/APDL2425a/group_12/gorodissky/.cache/huggingface\"\n",
    "print(f\"HF_HOME set to:\\t\\t {os.environ['HF_HOME']}\")\n",
    "\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from cisc.src.diis.probe import CorrectnessScorer\n",
    "\n",
    "print(f\"CUDA available: \\t{torch.cuda.is_available()}\")\n",
    "print(f\"Torch version: \\t\\t{torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of CUDA devices\\t {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device:\\t\\t {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556156f",
   "metadata": {},
   "source": [
    "## Choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62c1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "    model_name: str = \"Qwen/Qwen2.5-0.5B-Instruct\"  # \"google/gemma-2-2b-it\"  hf model\n",
    "    judge_model_name: str = \"gpt-4o-mini\"  # model used for judging answers\n",
    "    base_save_dir: str = \"/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/output\"  # base directory to save outputs\n",
    "    dataset_name: tuple[str, str] = (\n",
    "        \"mandarjoshi/trivia_qa\",\n",
    "        \"rc\",\n",
    "    )  # hf dataset name and subset\n",
    "    dataset_size: int = 16\n",
    "    seed: int = 1337\n",
    "    batch_size: int = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c7ea8",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6553908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_dataset():\n",
    "    global cfg\n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset: {cfg.dataset_name}\")\n",
    "    ds = load_dataset(*cfg.dataset_name, split=\"validation\", streaming=False)\n",
    "    ds = ds.shuffle(seed=cfg.seed).select(range(cfg.dataset_size))\n",
    "    print(f\"Dataset loaded: {len(ds)} examples\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"\\nLoading model: {cfg.model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    print(f\"Model loaded on device: {model.device} dtype: {model.dtype}\")\n",
    "    print(f\"Model has {model.config.num_hidden_layers} layers\")\n",
    "    return model, tokenizer, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e382ebd",
   "metadata": {},
   "source": [
    "## Set up activation capture for mid layers (50%) and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc7c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_hooks(model):\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    for layer_indx in range(num_layers):\n",
    "        model.model.layers[layer_indx]._forward_hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1692adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_activation_hooks(\n",
    "    model,\n",
    "    activations_storage_minus1,\n",
    "    activations_storage_mid,\n",
    "    activations_storage_plus1,\n",
    "):\n",
    "    # First cleanup\n",
    "    cleanup_hooks(model)\n",
    "    mid_layer_idx = model.config.num_hidden_layers // 2\n",
    "    layer_indices = [mid_layer_idx - 1, mid_layer_idx, mid_layer_idx + 1]\n",
    "    print(f\"Capturing activations from layers {layer_indices} (around 50% depth)\")\n",
    "\n",
    "    def activation_hook_minus1(module, input, output):\n",
    "        \"\"\"Hook to capture the last token's hidden state from layer mid-1\"\"\"\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # if output is a tuple, first element is hidden states\n",
    "        last_token_activation = output[:, -1, :].detach().cpu()\n",
    "        activations_storage_minus1.append(last_token_activation)\n",
    "\n",
    "    def activation_hook_mid(module, input, output):\n",
    "        \"\"\"Hook to capture the last token's hidden state from middle layer\"\"\"\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # if output is a tuple, first element is hidden states\n",
    "        last_token_activation = output[:, -1, :].detach().cpu()\n",
    "        activations_storage_mid.append(last_token_activation)\n",
    "\n",
    "    def activation_hook_plus1(module, input, output):\n",
    "        \"\"\"Hook to capture the last token's hidden state from layer mid+1\"\"\"\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # if output is a tuple, first element is hidden states\n",
    "        last_token_activation = output[:, -1, :].detach().cpu()\n",
    "        activations_storage_plus1.append(last_token_activation)\n",
    "\n",
    "    # Register hooks on the three layers\n",
    "    hook_handle_minus1 = model.model.layers[mid_layer_idx - 1].register_forward_hook(\n",
    "        activation_hook_minus1\n",
    "    )\n",
    "    hook_handle_mid = model.model.layers[mid_layer_idx].register_forward_hook(\n",
    "        activation_hook_mid\n",
    "    )\n",
    "    hook_handle_plus1 = model.model.layers[mid_layer_idx + 1].register_forward_hook(\n",
    "        activation_hook_plus1\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        activations_storage_minus1,\n",
    "        activations_storage_mid,\n",
    "        activations_storage_plus1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42ce85",
   "metadata": {},
   "source": [
    "## Generate responses and collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4fc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_collect_activations(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    ds,\n",
    "    activations_storage_minus1,\n",
    "    activations_storage_mid,\n",
    "    activations_storage_plus1,\n",
    "):\n",
    "    global cfg\n",
    "    print(f\"\\nGenerating responses with batch_size={cfg.batch_size}...\")\n",
    "\n",
    "    prompt_template = \"\"\"I am going to ask you a question. Answer concisely. End your sentence with {eos_token}. Here\n",
    "    are some examples of questions that might help you:\n",
    "    —--\n",
    "    Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
    "    Answer: Sinclair Lewis{eos_token}\n",
    "    —--\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    # DEBUG: print(prompt_template.format(eos_token=\"<|eos|>\", question=\"Which American-born Sinclair won the Nobel Prize for Literature in 1930?\"))\n",
    "    all_questions = []\n",
    "    all_ground_truths = []\n",
    "    all_model_answers = []\n",
    "    all_activations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(ds), cfg.batch_size)):\n",
    "        batch = ds[i : min(i + cfg.batch_size, len(ds))]\n",
    "\n",
    "        # Prepare questions\n",
    "        questions = batch[\"question\"]\n",
    "        all_questions.extend(questions)\n",
    "\n",
    "        # Store ground truth\n",
    "        ground_truths = [\n",
    "            ans[\"value\"][0] if isinstance(ans[\"value\"], list) else ans[\"value\"]\n",
    "            for ans in batch[\"answer\"]\n",
    "        ]\n",
    "        all_ground_truths.extend(ground_truths)\n",
    "\n",
    "        # Format prompts\n",
    "        prompts = [\n",
    "            prompt_template.format(eos_token=tokenizer.eos_token, question=q)\n",
    "            for q in questions\n",
    "        ]\n",
    "        prompts = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenizer.padding_side = \"left\"  # last token is the prompt's\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            prompts,\n",
    "            tokeinze=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # # DEBUG: print proccessed prompt\n",
    "        # print(f\"procced prompts shape: {inputs['input_ids'].shape}\")\n",
    "        # print(\"attention mask:\\n\", inputs['attention_mask'])\n",
    "        # procced_prompts =tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=False)\n",
    "        # print(\"procced prompts:\")\n",
    "        # for pp in procced_prompts:\n",
    "        #     print(repr(pp))\n",
    "        # break\n",
    "\n",
    "        # Clear previous activations\n",
    "        activations_storage_minus1.clear()\n",
    "        activations_storage_mid.clear()\n",
    "        activations_storage_plus1.clear()\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=False,  # greedy decoding matching the paper's setup\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # # DEBUG: print responses\n",
    "        # print(type(outputs))\n",
    "        # answers_tokens = outputs[:, inputs[\"input_ids\"].size(1):]\n",
    "        # answers = tokenizer.batch_decode(answers_tokens, skip_special_tokens=False)\n",
    "        # for a in answers:\n",
    "        #     print(a)\n",
    "        # break\n",
    "\n",
    "        # Extract answers (remove prompt)\n",
    "        prompt_lengths = inputs[\"input_ids\"].shape[1]\n",
    "        generated_tokens = outputs[:, prompt_lengths:]\n",
    "        answers = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        all_model_answers.extend(answers)\n",
    "\n",
    "        # Store activations from the FIRST forward pass (the prompt processing)\n",
    "        # The first forward pass processes the full prompt and the hooks captures the last token activation\n",
    "        # The subsequent forward passes proccess the new generated tokens only each time sequence of length 1\n",
    "        if (\n",
    "            activations_storage_minus1\n",
    "            and activations_storage_mid\n",
    "            and activations_storage_plus1\n",
    "        ):\n",
    "            act_minus1 = activations_storage_minus1[0]\n",
    "            act_mid = activations_storage_mid[0]\n",
    "            act_plus1 = activations_storage_plus1[0]\n",
    "\n",
    "            # Compute mean across the 3 layers\n",
    "            mean_activation = torch.stack([act_minus1, act_mid, act_plus1], dim=0).mean(\n",
    "                dim=0\n",
    "            )\n",
    "            all_activations.append(mean_activation)\n",
    "\n",
    "    # Remove hooks\n",
    "    cleanup_hooks(model)\n",
    "\n",
    "    # Concatenate all activations\n",
    "    activations_tensor = torch.cat(all_activations, dim=0)\n",
    "    print(f\"\\nCollected {len(all_questions)} Q&A pairs\")\n",
    "    print(\n",
    "        f\"Activations shape: {activations_tensor.shape} (Mean of middle layers from prompt's last token)\"\n",
    "    )\n",
    "    return (\n",
    "        all_questions,\n",
    "        all_ground_truths,\n",
    "        all_model_answers,\n",
    "        activations_tensor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e329d",
   "metadata": {},
   "source": [
    "## LLM as a judge to label correct/incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597941c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_with_judge(all_questions, all_ground_truths, all_model_answers):\n",
    "    global cfg\n",
    "    with open(\n",
    "        \"/home/yandex/APDL2425a/group_12/gorodissky/tokens/openai_key_personal.txt\", \"r\"\n",
    "    ) as f:\n",
    "        openai_key = f.read().strip()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "    def query_gpt(prompt, model=\"gpt-4.1-nano\"):\n",
    "        client = openai.OpenAI()\n",
    "\n",
    "        response = client.responses.create(model=model, input=prompt)\n",
    "        text = response.output_text.strip()\n",
    "        return text\n",
    "\n",
    "    print(\"\\nEvaluating answers using LLM as judge...\")\n",
    "\n",
    "    judge_prompt_template = \"\"\"You are evaluating whether a model's answer is correct.\n",
    "\n",
    "    Question: {question}\n",
    "    Ground Truth: {ground_truth}\n",
    "    Model Answer: {model_answer}\n",
    "\n",
    "    Is the model's answer correct? Consider semantic equivalence, not just exact match. Think step by step.\n",
    "    Respond with only \"CORRECT\" or \"INCORRECT\".\n",
    "\n",
    "    Judgment:\"\"\"\n",
    "\n",
    "    correctness_labels = []\n",
    "\n",
    "    for i, (q, gt, ma) in enumerate(\n",
    "        tqdm(\n",
    "            zip(all_questions, all_ground_truths, all_model_answers),\n",
    "            total=len(all_questions),\n",
    "            desc=\"Judging\",\n",
    "        )\n",
    "    ):\n",
    "        judge_prompt = judge_prompt_template.format(\n",
    "            question=q, ground_truth=gt, model_answer=ma\n",
    "        )\n",
    "        judgment = query_gpt(judge_prompt, model=cfg.judge_model_name)\n",
    "        is_correct = (\n",
    "            \"CORRECT\" in judgment.upper() and \"INCORRECT\" not in judgment.upper()\n",
    "        )\n",
    "        # # DEBUG: print prompt and judgment\n",
    "        # print(\"\\n--- Judge Prompt ---\")\n",
    "        # print(judge_prompt)\n",
    "        # print(\"--- Judgment ---\")\n",
    "        # print(judgment)\n",
    "        # print(f\"Is correct: {is_correct}\")\n",
    "        correctness_labels.append(is_correct)\n",
    "\n",
    "    correctness_array = np.array(correctness_labels)\n",
    "    num_correct = correctness_array.sum()\n",
    "    num_incorrect = len(correctness_array) - num_correct\n",
    "\n",
    "    print(\n",
    "        f\"\\nCorrect: {num_correct} ({100 * num_correct / len(correctness_array):.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Incorrect: {num_incorrect} ({100 * num_incorrect / len(correctness_array):.1f}%)\"\n",
    "    )\n",
    "    return correctness_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fa05a",
   "metadata": {},
   "source": [
    "## Calculate correctness direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b375d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correctness_direction(\n",
    "    activations_tensor: torch.Tensor, correctness_array: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    print(\"\\nCalculating probe direction...\")\n",
    "\n",
    "    # Convert to numpy for easier computation\n",
    "    activations_np = activations_tensor.to(torch.float32).numpy()\n",
    "\n",
    "    # Partition into correct and incorrect\n",
    "    correct_mask = correctness_array\n",
    "    incorrect_mask = ~correctness_array\n",
    "\n",
    "    correct_activations = activations_np[correct_mask]\n",
    "    incorrect_activations = activations_np[incorrect_mask]\n",
    "\n",
    "    print(f\"Correct activations: {correct_activations.shape}\")\n",
    "    print(f\"Incorrect activations: {incorrect_activations.shape}\")\n",
    "\n",
    "    # Calculate centroids\n",
    "    correct_centroid = correct_activations.mean(axis=0)\n",
    "    incorrect_centroid = incorrect_activations.mean(axis=0)\n",
    "\n",
    "    # New origin is the midpoint between centroids\n",
    "    new_origin = (correct_centroid + incorrect_centroid) / 2\n",
    "\n",
    "    # Translate to new origin\n",
    "    correct_centered = correct_activations - new_origin\n",
    "    incorrect_centered = incorrect_activations - new_origin\n",
    "\n",
    "    # Calculate mean directions from new origin\n",
    "    mu_correct = correct_centered.mean(axis=0)\n",
    "    mu_incorrect = incorrect_centered.mean(axis=0)\n",
    "\n",
    "    # Direction from incorrect to correct\n",
    "    direction = 0.5 * (mu_correct - mu_incorrect)\n",
    "\n",
    "    print(f\"\\nDirection vector shape: {direction.shape}\")\n",
    "    print(f\"Direction vector norm: {np.linalg.norm(direction):.4f}\")\n",
    "\n",
    "    return CorrectnessScorer(direction, new_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75beab",
   "metadata": {},
   "source": [
    "## Calculate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e40a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcluate_metadata(scorer, activations_tensor, correctness_array, model):\n",
    "    global cfg\n",
    "    print(\"\\nEvaluating scorer on training data...\")\n",
    "\n",
    "    # Convert to numpy for easier computation\n",
    "    activations_np = activations_tensor.to(torch.float32).numpy()\n",
    "\n",
    "    # Score all activations\n",
    "    scores = scorer.score(activations_np)\n",
    "\n",
    "    # Analyze score distribution\n",
    "    correct_mask = correctness_array\n",
    "    incorrect_mask = ~correctness_array\n",
    "    correct_scores = scores[correct_mask]\n",
    "    incorrect_scores = scores[incorrect_mask]\n",
    "\n",
    "    print(f\"\\nCorrect answers:\")\n",
    "    print(f\"  Mean score: {correct_scores.mean():.4f}\")\n",
    "    print(f\"  Std score: {correct_scores.std():.4f}\")\n",
    "\n",
    "    print(f\"\\nIncorrect answers:\")\n",
    "    print(f\"  Mean score: {incorrect_scores.mean():.4f}\")\n",
    "    print(f\"  Std score: {incorrect_scores.std():.4f}\")\n",
    "\n",
    "    # Calculate separation\n",
    "    separation = np.abs(correct_scores.mean() - incorrect_scores.mean())\n",
    "    print(f\"\\nSeparation (distance between means): {separation:.4f}\")\n",
    "\n",
    "    # Simple threshold classification (at midpoint)\n",
    "    threshold = (correct_scores.mean() + incorrect_scores.mean()) / 2\n",
    "    predicted_correct = scores > threshold\n",
    "    accuracy = (predicted_correct == correctness_array).mean()\n",
    "    print(f\"Training accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    mid_layer_idx = model.config.num_hidden_layers // 2\n",
    "    layer_indices = [mid_layer_idx - 1, mid_layer_idx, mid_layer_idx + 1]\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": cfg.model_name,\n",
    "        \"dataset_name\": cfg.dataset_name,\n",
    "        \"dataset_size\": cfg.dataset_size,\n",
    "        \"seed\": cfg.seed,\n",
    "        \"layers_idx\": layer_indices,\n",
    "        \"correct_score_mean\": float(correct_scores.mean()),\n",
    "        \"correct_score_std\": float(correct_scores.std()),\n",
    "        \"incorrect_score_mean\": float(incorrect_scores.mean()),\n",
    "        \"incorrect_score_std\": float(incorrect_scores.std()),\n",
    "        \"separation\": float(separation),\n",
    "        \"training_accuracy\": float(accuracy),\n",
    "        \"threshold (midpoint between means)\": float(threshold),\n",
    "    }\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ba1ec",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1412be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(scorer, metadata):\n",
    "    global cfg\n",
    "    save_dir = Path(cfg.base_save_dir)\n",
    "    save_dir = save_dir / \"probe_results\" / cfg.dataset_name[0] / cfg.model_name\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the scorer\n",
    "    version = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "    scorer_path = save_dir / f\"correctness_scorer_{version}.npz\"\n",
    "    scorer.save(str(scorer_path))\n",
    "\n",
    "    # Save metadata and results\n",
    "\n",
    "    metadata_path = save_dir / f\"metadata_{version}.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\" Results saved to {save_dir}\")\n",
    "    print(f\"  - Scorer: {scorer_path}\")\n",
    "    print(f\"  - Metadata: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd5ed7",
   "metadata": {},
   "source": [
    "# Run whole pipline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d530ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Processing model: Qwen/Qwen2.5-7B-Instruct ===\n",
      "Loading dataset: ('mandarjoshi/trivia_qa', 'rc')\n",
      "Dataset loaded: 8192 examples\n",
      "\n",
      "Loading model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 18:44:50.204626: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-04 18:45:00.724586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-04 18:45:15.147495: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/yandex/APDL2425a/group_12/gorodissky/google-research/cisc/diis-venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [05:48<00:00, 87.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0 dtype: torch.bfloat16\n",
      "Model has 28 layers\n",
      "Capturing activations from layers [13, 14, 15] (around 50% depth)\n",
      "\n",
      "Generating responses with batch_size=8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    global cfg\n",
    "    cfg = config()\n",
    "    cfg.dataset_size = int(2**13)\n",
    "    cfg.batch_size = 8\n",
    "    for model_name in [\n",
    "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"google/gemma-3-4b-it\",\n",
    "        \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"mistralai/Ministral-8B-Instruct-2410\",\n",
    "    ]:\n",
    "        print(f\"\\n\\n=== Processing model: {model_name} ===\")\n",
    "        cfg.model_name = model_name\n",
    "        model, tokenizer, ds = load_model_and_dataset()\n",
    "\n",
    "        # Prepare activation storage\n",
    "        activations_storage_minus1: List[torch.Tensor] = []\n",
    "        activations_storage_mid: List[torch.Tensor] = []\n",
    "        activations_storage_plus1: List[torch.Tensor] = []\n",
    "\n",
    "        # Register hooks\n",
    "        (\n",
    "            activations_storage_minus1,\n",
    "            activations_storage_mid,\n",
    "            activations_storage_plus1,\n",
    "        ) = register_activation_hooks(\n",
    "            model,\n",
    "            activations_storage_minus1,\n",
    "            activations_storage_mid,\n",
    "            activations_storage_plus1,\n",
    "        )\n",
    "\n",
    "        # Generate and collect activations\n",
    "        (\n",
    "            all_questions,\n",
    "            all_ground_truths,\n",
    "            all_model_answers,\n",
    "            activations_tensor,\n",
    "        ) = generate_and_collect_activations(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            ds,\n",
    "            activations_storage_minus1,\n",
    "            activations_storage_mid,\n",
    "            activations_storage_plus1,\n",
    "        )\n",
    "\n",
    "        # Label with judge\n",
    "        correctness_array = label_with_judge(\n",
    "            all_questions, all_ground_truths, all_model_answers\n",
    "        )\n",
    "\n",
    "        # Calculate correctness direction\n",
    "        scorer = calculate_correctness_direction(activations_tensor, correctness_array)\n",
    "\n",
    "        # Calculate metadata\n",
    "        metadata = calcluate_metadata(\n",
    "            scorer, activations_tensor, correctness_array, model\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        save(scorer, metadata)\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86839744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIIS venv",
   "language": "python",
   "name": "diis-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
